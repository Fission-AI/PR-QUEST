import { describe, expect, it, vi, beforeEach, afterAll } from "vitest";

import { POST } from "../route";
import { buildHeuristicReviewPlan } from "@/lib/heuristic-steps";
import type { DiffIndex } from "@/lib/diff-index";
import type { ReviewPlan } from "@/lib/review-plan-schema";
import { buildLlmReviewPlan } from "@/lib/llm-steps";

vi.mock("@/lib/llm-steps", () => ({
  buildLlmReviewPlan: vi.fn(),
}));

const SAMPLE_DIFF_INDEX: DiffIndex = {
  diff_index_version: 1,
  files: [
    {
      file_id: "src/app/page.tsx",
      status: "modified",
      language: "tsx",
      hunks: [
        {
          hunk_id: "src/app/page.tsx#h0",
          old_start: 5,
          new_start: 5,
          header: "@@ -5,6 +5,9 @@",
        },
      ],
    },
  ],
};

const ORIGINAL_FLAG = process.env.FEATURE_USE_LLM;

beforeEach(() => {
  vi.clearAllMocks();
  delete process.env.FEATURE_USE_LLM;
});

afterAll(() => {
  if (ORIGINAL_FLAG === undefined) {
    delete process.env.FEATURE_USE_LLM;
    return;
  }

  process.env.FEATURE_USE_LLM = ORIGINAL_FLAG;
});

describe("POST /api/group", () => {
  it("returns a 400 when the payload is not valid JSON", async () => {
    const request = new Request("http://localhost/api/group", {
      method: "POST",
      body: "not-json",
      headers: {
        "Content-Type": "application/json",
      },
    });

    const response = await POST(request);

    expect(response.status).toBe(400);
    expect(await response.json()).toMatchObject({
      error: "Request body must be valid JSON.",
    });
  });

  it("returns the heuristic plan when the feature flag is disabled", async () => {
    process.env.FEATURE_USE_LLM = "false";
    const metadata = { prTitle: "Tweak hero copy", prDescription: "Minor copy edits." };
    const heuristicPlan = buildHeuristicReviewPlan({
      diffIndex: SAMPLE_DIFF_INDEX,
      prTitle: metadata.prTitle,
      prDescription: metadata.prDescription,
    });

    const request = new Request("http://localhost/api/group", {
      method: "POST",
      body: JSON.stringify({
        diffIndex: SAMPLE_DIFF_INDEX,
        metadata,
      }),
    });

    const response = await POST(request);

    expect(response.status).toBe(200);
    expect(response.headers.get("x-pr-quest-grouping-mode")).toBe("heuristic");
    const body = await response.json();
    expect(body).toEqual(heuristicPlan);
  });

  it("invokes the LLM planner when the feature flag is enabled", async () => {
    const mockLlmPlan: ReviewPlan = {
      version: 1,
      pr_overview: {
        title: "AI plan",
        summary: "Generated by the LLM planner.",
      },
      steps: [
        {
          step_id: "step-1",
          title: "Review hero text adjustments",
          description: "Ensure updated copy matches marketing guidelines.",
          objective: "Confirm new call-to-action copy is accurate.",
          priority: "low",
          diff_refs: [
            {
              file_id: "src/app/page.tsx",
              hunk_ids: ["src/app/page.tsx#h0"],
            },
          ],
          notes_suggested: [],
          badges: ["Docs"],
        },
      ],
      end_state: {
        acceptance_checks: ["Verify copy renders correctly in the browser."],
        risk_calls: ["Copy regression might confuse returning users."],
      },
    };

    process.env.FEATURE_USE_LLM = "true";
    const metadata = { prTitle: "Update hero copy" };
    const heuristicPlan = buildHeuristicReviewPlan({
      diffIndex: SAMPLE_DIFF_INDEX,
      prTitle: metadata.prTitle,
    });

    vi.mocked(buildLlmReviewPlan).mockResolvedValue(mockLlmPlan);

    const request = new Request("http://localhost/api/group", {
      method: "POST",
      body: JSON.stringify({
        diffIndex: SAMPLE_DIFF_INDEX,
        metadata,
      }),
    });

    const response = await POST(request);

    expect(response.status).toBe(200);
    expect(response.headers.get("x-pr-quest-grouping-mode")).toBe("llm");
    expect(await response.json()).toEqual(mockLlmPlan);

    expect(buildLlmReviewPlan).toHaveBeenCalledTimes(1);
    const callArgs = vi.mocked(buildLlmReviewPlan).mock.calls[0]?.[0];
    expect(callArgs).toBeDefined();
    expect(callArgs?.baselinePlan).toEqual(heuristicPlan);
    expect(callArgs?.metadata).toEqual({ prTitle: "Update hero copy" });
  });

  it("falls back to the heuristic plan when the LLM throws", async () => {
    process.env.FEATURE_USE_LLM = "true";
    const metadata = {};
    const heuristicPlan = buildHeuristicReviewPlan({
      diffIndex: SAMPLE_DIFF_INDEX,
    });

    const consoleSpy = vi.spyOn(console, "error").mockImplementation(() => {});
    vi.mocked(buildLlmReviewPlan).mockRejectedValue(new Error("provider down"));

    const request = new Request("http://localhost/api/group", {
      method: "POST",
      body: JSON.stringify({
        diffIndex: SAMPLE_DIFF_INDEX,
        metadata,
      }),
    });

    const response = await POST(request);

    expect(response.status).toBe(200);
    expect(response.headers.get("x-pr-quest-grouping-mode")).toBe("llm-fallback");
    expect(await response.json()).toEqual(heuristicPlan);
    consoleSpy.mockRestore();
  });
});
